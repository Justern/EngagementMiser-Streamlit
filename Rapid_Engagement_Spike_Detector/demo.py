#!/usr/bin/env python3
"""
Demo script for Rapid Engagement Spike Detector

This script demonstrates the full functionality of the model using real database data.
"""

from rapid_engagement_spike_detector import RapidEngagementSpikeDetector
from data_connector import EngagementDataConnector
import pandas as pd

def demo_spike_detection():
    """Demonstrate spike detection with real database data."""
    print("ğŸš€ RAPID ENGAGEMENT SPIKE DETECTOR DEMO")
    print("=" * 60)
    
    try:
        # Initialize components
        print("ğŸ”§ Initializing components...")
        detector = RapidEngagementSpikeDetector()
        connector = EngagementDataConnector()
        
        # Test database connection
        print("ğŸ”Œ Testing database connection...")
        if not connector.test_connection():
            print("âŒ Database connection failed!")
            return
        
        print("âœ… Database connection successful!")
        
        # Get sample data
        print("\nğŸ“¥ Retrieving sample engagement data...")
        sample_data = connector.get_sample_engagement_data(limit=200)
        
        if sample_data.empty:
            print("âŒ No sample data retrieved")
            return
        
        print(f"âœ… Retrieved {len(sample_data)} sample records")
        print(f"ğŸ“Š Data range: {sample_data['timestamp'].min()} to {sample_data['timestamp'].max()}")
        print(f"ğŸ“ˆ Engagement range: {sample_data['total_engagements'].min()} to {sample_data['total_engagements'].max()}")
        
        # Find a tweet with good engagement for analysis
        print("\nğŸ” Finding tweet for detailed analysis...")
        high_engagement_tweets = sample_data[sample_data['total_engagements'] > 50]
        
        if high_engagement_tweets.empty:
            print("âš ï¸  No high engagement tweets found, using random sample")
            analysis_tweet = sample_data.iloc[0]
        else:
            analysis_tweet = high_engagement_tweets.iloc[0]
        
        tweet_id = analysis_tweet['tweet_id']
        print(f"ğŸ“ Selected tweet ID: {tweet_id}")
        print(f"ğŸ“Š Engagement: {analysis_tweet['total_engagements']}")
        print(f"ğŸ“… Created: {analysis_tweet['timestamp']}")
        print(f"ğŸ“ Text: {analysis_tweet['text'][:100]}...")
        
        # Get engagement timeline for this tweet
        print(f"\nğŸ“ˆ Retrieving engagement timeline for tweet {tweet_id}...")
        timeline_data = connector.get_tweet_engagement_timeline(tweet_id, hours_back=24)
        
        if timeline_data.empty:
            print("âš ï¸  No timeline data found, using sample data for demonstration")
            # Use the sample data as a proxy for timeline
            timeline_data = sample_data.copy()
            timeline_data['tweet_id'] = tweet_id
        
        print(f"âœ… Timeline data: {len(timeline_data)} data points")
        
        # Analyze the engagement patterns
        print(f"\nğŸ” Analyzing engagement patterns for spike detection...")
        results = detector.analyze_tweet_engagement(tweet_id, timeline_data)
        
        if 'error' in results:
            print(f"âŒ Analysis error: {results['error']}")
            return
        
        # Display comprehensive results
        print(f"\nğŸ“Š COMPREHENSIVE SPIKE DETECTION RESULTS")
        print("=" * 60)
        
        # Main results
        print(f"ğŸ¯ Tweet ID: {results['tweet_id']}")
        print(f"ğŸ“Š Spike Detection Score: {results['spike_detection_score']:.3f}")
        print(f"ğŸ” Spike Level: {results['spike_level']}")
        print(f"ğŸ“ˆ Data Points Analyzed: {results['data_points']}")
        
        # Time range
        time_range = results['time_range']
        print(f"â° Analysis Time Range: {time_range['start']} to {time_range['end']}")
        print(f"â±ï¸  Duration: {time_range['duration_hours']:.1f} hours")
        
        # Anomaly detection
        anomaly_results = results['anomaly_detection']
        print(f"\nğŸš¨ ANOMALY DETECTION:")
        print(f"  ğŸ“Š Anomaly Score: {anomaly_results['anomaly_score']:.3f}")
        print(f"  ğŸ” Anomalies Found: {anomaly_results['anomaly_count']}")
        
        if anomaly_results['anomalies']:
            print("  ğŸ“‹ Top Anomalies:")
            for i, anomaly in enumerate(anomaly_results['anomalies'][:3]):
                print(f"    {i+1}. Time: {anomaly['timestamp']}, Score: {anomaly['anomaly_score']:.3f}")
        
        # Spike detection
        spike_results = results['spike_detection']
        print(f"\nâš¡ SPIKE DETECTION:")
        print(f"  ğŸ“Š Spike Score: {spike_results['spike_score']:.3f}")
        print(f"  ğŸ” Spikes Found: {spike_results['spike_count']}")
        
        if spike_results['spikes']:
            print("  ğŸ“‹ Top Spikes:")
            for i, spike in enumerate(spike_results['spikes'][:3]):
                print(f"    {i+1}. Time: {spike['timestamp']}, Method: {spike['method']}, Score: {spike['spike_score']:.3f}")
        
        # Changepoint detection
        changepoint_results = results['changepoint_detection']
        print(f"\nğŸ”„ CHANGEPOINT DETECTION:")
        print(f"  ğŸ“Š Changepoint Score: {changepoint_results['changepoint_score']:.3f}")
        print(f"  ğŸ” Changepoints Found: {changepoint_results['changepoint_count']}")
        
        if changepoint_results['changepoints']:
            print("  ğŸ“‹ Changepoints:")
            for i, changepoint in enumerate(changepoint_results['changepoints'][:3]):
                print(f"    {i+1}. Time: {changepoint['timestamp']}, Index: {changepoint['index']}")
        
        # Pattern analysis
        pattern_results = results['pattern_analysis']
        print(f"\nğŸ“Š PATTERN ANALYSIS:")
        print(f"  ğŸ“ˆ Trend Direction: {pattern_results.get('trend_direction', 'N/A')}")
        print(f"  ğŸ“Š Volatility Level: {pattern_results.get('volatility_level', 'N/A')}")
        print(f"  ğŸ”¢ Total Engagement: {pattern_results.get('total_engagement', 0):,}")
        print(f"  ğŸ“Š Mean Engagement: {pattern_results.get('mean_engagement', 0):.1f}")
        print(f"  ğŸ“Š Std Engagement: {pattern_results.get('std_engagement', 0):.1f}")
        print(f"  ğŸ“Š Pattern Score: {pattern_results.get('pattern_score', 0):.3f}")
        
        # Score interpretation
        print(f"\nğŸ¯ SCORE INTERPRETATION:")
        score = results['spike_detection_score']
        if score >= 0.7:
            interpretation = "HIGH_SPIKE - Very likely to have rapid engagement spikes"
            recommendation = "Monitor closely for viral content or coordinated activity"
        elif score >= 0.4:
            interpretation = "MODERATE_SPIKE - Likely to have some engagement spikes"
            recommendation = "Watch for unusual engagement patterns"
        elif score >= 0.2:
            interpretation = "LOW_SPIKE - Minimal engagement spikes detected"
            recommendation = "Normal engagement patterns"
        else:
            interpretation = "NO_SPIKE - No significant engagement spikes detected"
            recommendation = "Stable, organic engagement"
        
        print(f"  ğŸ” {interpretation}")
        print(f"  ğŸ’¡ Recommendation: {recommendation}")
        
        # Show sample of raw data
        if results.get('raw_data'):
            print(f"\nğŸ“‹ SAMPLE RAW DATA (First 5 records):")
            raw_df = pd.DataFrame(results['raw_data'])
            print(raw_df[['timestamp', 'total_engagements', 'likes', 'retweets', 'replies']].head())
        
        print(f"\nâœ… Demo completed successfully!")
        print(f"ğŸ¯ Final Spike Detection Score: {score:.3f}")
        
    except Exception as e:
        print(f"âŒ Demo error: {str(e)}")
        import traceback
        traceback.print_exc()

def demo_conversation_analysis():
    """Demonstrate conversation-level spike detection."""
    print("\nğŸ”— CONVERSATION SPIKE DETECTION DEMO")
    print("=" * 60)
    
    try:
        # Initialize components
        detector = RapidEngagementSpikeDetector()
        connector = EngagementDataConnector()
        
        # Get sample data to find a conversation
        sample_data = connector.get_sample_engagement_data(limit=500)
        
        if sample_data.empty:
            print("âŒ No sample data for conversation analysis")
            return
        
        # Find tweets with conversation IDs
        conversations = sample_data[sample_data['conversation_id'].notna()]
        
        if conversations.empty:
            print("âš ï¸  No conversation data found")
            return
        
        # Get a conversation ID
        conversation_id = conversations.iloc[0]['conversation_id']
        print(f"ğŸ”— Analyzing conversation: {conversation_id}")
        
        # Get conversation timeline
        conv_data = connector.get_conversation_engagement_timeline(conversation_id, hours_back=48)
        
        if conv_data.empty:
            print("âš ï¸  No conversation timeline data")
            return
        
        print(f"âœ… Conversation data: {len(conv_data)} tweets")
        print(f"ğŸ“… Time range: {conv_data['timestamp'].min()} to {conv_data['timestamp'].max()}")
        
        # Analyze conversation
        results = detector.analyze_tweet_engagement(conversation_id, conv_data)
        
        if 'error' in results:
            print(f"âŒ Conversation analysis error: {results['error']}")
            return
        
        print(f"\nğŸ“Š CONVERSATION ANALYSIS RESULTS:")
        print(f"ğŸ¯ Conversation ID: {conversation_id}")
        print(f"ğŸ“Š Spike Detection Score: {results['spike_detection_score']:.3f}")
        print(f"ğŸ” Spike Level: {results['spike_level']}")
        print(f"ğŸ“ˆ Tweets in Conversation: {results['data_points']}")
        print(f"ğŸš¨ Anomalies: {results['anomaly_detection']['anomaly_count']}")
        print(f"âš¡ Spikes: {results['spike_detection']['spike_count']}")
        print(f"ğŸ”„ Changepoints: {results['changepoint_detection']['changepoint_count']}")
        
    except Exception as e:
        print(f"âŒ Conversation demo error: {str(e)}")

if __name__ == "__main__":
    # Run main demo
    demo_spike_detection()
    
    # Run conversation demo
    demo_conversation_analysis()
    
    print(f"\nğŸ‰ All demos completed!")
    print(f"ğŸ’¡ To use the model interactively, run: python simple_usage.py")
